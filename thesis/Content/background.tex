\chapter{Background}
\label{chap:background}
\todoWriteMore{1st paper and its limitations (my own ideas: add also things about: supervised learning, tree models, boosting, gradient boosting, loss functions in gradient boosting, PU formulation, loss functions in gradient boosting, PU formulation, loss functions for PU problems (see paper)}
\todoWriteMore{!!!restructure!!!}

This chapter will briefly explain the method of Vilari\~no et al. to use gaze-tracking for polyp detection as well as elaborate their assumptions, limitations and conceptual problems that arise when trying to generalize their methods to arbitrary data, namely, other data than colonoscopy video. Further, there will be given a brief overview and explanation of the used techniques during this project.


\section{Previous work}
\subsection{Goal of Vilari\~no et al.}
In 2007, Vilari\~no et al. published a method that used gaze-tracking for polyp detection ( \todoRef{reference...}). The core idea of Vilari\~no et al. was to train a classifier using expert's gaze positions to generate positive and negative training samples, instead of manually labeled training data. 
The availability of labeled training data has been the bottleneck for enabling Machine Learning and Computer Vision methods for clinical applications for the last \todo{\#years?} years. 
The idea of Vilari\~no et al. has a great potential to tackle this issue because it's aimed at reducing the time costs to create training data for classifiers.

\subsection{Assumptions}
Vilari\~no et al. made assumptions about gaze positions and their associations with salient features of the image. 
In this project, we elaborated whether or not these assumptions are appropriate, and for what datasets. 
Also, they made implicit assumptions about the number and size of interesting objects in their data, namely that there is at most one object of interest in a time frame and that it fits within a patch of $128 \times 128$ pixels.

\subsection{Evaluation}
Their training data: 80\% of the ``gaze-labeled'' ROIs. Their test data: 20\% of the ``gaze-labeled'' ROIs. They didn't compare to any ``real'' ground truth, but basically only evaluated how well the SVM separates ``observed'' vs. ``not observed'' regions \todo{how do we do in this case?} -- however, this evaluation does not take into account that the generated labels from the gaze positions are inherently noisy.


\section{Initial position (case of Vilari\~no et al.)}
The application of Vilari\~no et al. was limited to polyp detection in colonoscopy videos. In their particular case, it seems that the following two crucial implicit assumption were fulfilled:
\begin{enumerate}
 \item in each video frame, there was at most one structure of interest (polyp)
 \item a structure of interest never exceeded the size of $128 \times 128$ pixels
\end{enumerate}
Examples of how their data looked like are shown in Figure \ref{fig:vilarinoPolypExamples}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{vilarino-polyp-examples}
	\caption{heat map of the resulting scores with a SVM trained on all the labeled samples }
	\label{fig:vilarinoPolypExamples}
\end{figure}


\section{Generalization to wider applications / datasets -- problems}
Using datasets that fulfill assumptions 1 and 2 from above, we could achieve quite some reasonable results with Vilari\~no's approach regarding detection (Figure \ref{fig:theirapproachairplane}). As we are aiming for a pixel-/voxelwise segmentation of our input, we have to capture the object boundaries and regarding this task, we could significantly improve the quality of the output by using color-based features \todo{perhaps specify} of SLIC superpixels instead of the values of pre-processed image patches to describe image regions. The results are visible in Figure \ref{fig:airplaneSLIC}. To get a useful binary result, we could not use all of the negatively labeled superpixels because the SVM classifier could not handle the class imbalance, as can be seen from the heat map in Figure \ref{fig:airplaneSVMclassImbalance}: Even though the aircraft clearly gets higher scores than the background, they remain below zero. Therefore we decided to work with a gradient boosting technique as described in Chapter \ref{chap:background}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00189}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 189)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame189-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00249}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 249)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame249-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}	
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame249-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}	
	\caption{inputs and outputs obtained with Vilari\~no's approach. For the shown examples, we used a SVM classifier ({\tt libsvm} package for MATLAB) with a RBF kernel ($\gamma = 0.625$) and a rather high regularization value of $c = 10$. Note that the region containing the airplane was at least partly detected.}
	\label{fig:theirapproachairplane}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00189}
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 189)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00249}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 249)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}	
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}	
	\caption{inputs and outputs obtained with color-based features of SLIC superpixels. We used the same classifier as in Figure \ref{fig:theirapproachairplane}, but used only a subset of the negatively labeled superpixels for training, as the SVM could not handle the class imbalance (see Figure \ref{fig:airplaneSVMclassImbalance}). }
	\label{fig:airplaneSLIC}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-c10-wholeTrainingSet}
	\caption{heat map of the resulting scores with a SVM trained on all the labeled samples }
	\label{fig:airplaneSVMclassImbalance}
\end{figure}


Datasets that do not fulfill both assumptions cause conceptual and practical problems. In the ideal case we have, for each frame, one true positive patch / superpixel given by the recorded gaze position. However, the assumption that all the other non-overlapping patches / superpixels are negative, is not valid, if one of the above-mentioned assumptions is not fulfilled. Figure \ref{fig:nonValidAssumption} shows a dataset containing a surgical instrument with a bigger extent. It is clearly visible that the negatively labeled patches / superpixels are not necessarily true negatives.



\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset2gazePositionFrame207}
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{frame 207 (red: gaze position)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	    \includegraphics[width=\textwidth]{dataset2SLICsegmentationFrame207}
	    %\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
	    \caption*{SLIC superpixels (red: gaze position)}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{dataset2positivePatchFrame207}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{positive patch}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{dataset2positiveSuperpixelFrame207}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{positive superpixel}
	\end{subfigure}
	
	\vspace{3mm}
		\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset2negativePatchesFrame207}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{``negative'' patches}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset2negativeSuperpixelsFrame207}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{``negative'' superpixels}
	\end{subfigure}	
	\caption{blah blah yaddah yaddah...). }
	\label{fig:nonValidAssumption}
\end{figure}




\todoWriteMore{show some positive and ``negative'' patches / superpixels like in 1st presentation}
If we consider one $128 \times 128$-patch / superpixel positive and all the other non-overlapping ones as negative examples, we run into mainly 2 issues: 
\begin{enumerate}
 \item unbalanced dataset (few positives, many negatives)
 \item very high likelihood that each positive sample has a corresondance in the negative training set.
\end{enumerate}
This leads to the optimal solution being a negative label for each test sample.

A natural step is therefore to formulate the problem in a different way, namely not in ``separate positive from negative samples'', but instead as ``given some positive samples, figure out whether or not the other, unlabeled ones, also belong to the positive or instead to the negative class.'' \todo{maybe illustration?}
\todo{somewhere here... split in chap 2 and chap 4; chap 4 should start only here!}
This so-called PU-problem recently got a lot of attention and applications in document classification, ... \todoRef{reference}.


\section{Classifiers and Boosting}
\todoWriteMore{give some basics about classifiers, and about boosting}
