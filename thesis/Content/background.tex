\chapter{Background}
\label{chap:background}
This chapter will briefly explain the method of Vilari\~no et al. to use gaze-tracking for polyp detection as well as elaborate their assumptions, limitations and conceptual problems that arise when trying to generalize their methods to arbitrary data gained through medical imaging methods. Further, there will be given a brief overview and explanation of classification in general and the used gradient boosting method.

\section{Classifiers and Boosting}
In classification, the task is to find for a given data sample $x \in \mathbb{R}^n$ a class label $f(x) \in \{c_1, c_2, ..., c_k\}$. As an example, $x$ could be a representation of an e-mail and $\{c_1,c_2,c_3\}$ could be \{``spam'', ``business'', ``private''\}. For simplicity, usually a binary classifier with $f(x) \in \{-1,1\}$ is considered and multiclass problems are later formulated in terms of the two-class approach. 
Given the true label $y$ for a sample $x$ and the prediction $f(x)$, the concept of a ``margin'' describes the confidence of the made prediction $f(x).$ 
It is foften defined as $z = y f(x)$.
Note that, as described until now, the margin would be
\begin{equation*}
z = 
     \begin{cases}
	+1, \quad \text{if } y = f(x), \\
	-1 \quad \text{if } y \neq f(x),
      \end{cases}
\end{equation*}
indicating that a negative value of the margin means that the prediction $f(x)$ was wrong. Usually, instead of $f: \mathbb{R}^n \longrightarrow \{-1,1\}$ we have a function $g$ predicting ``scores'' of a sample $x$, and depending on those scores, the function $f$ decides the label (e.g.\ $f(x) = \text{sign}(g(x))$). The margin is then calculated for the function $g$, the meaning staying the same: A large margin represents a confident decision, whereas a margin close to $0$ means a very insecure decision. A negative margin represents a wrong decision.

Finding the function $f^*$ that optimally assigns positive and negative class labels to all possible inputs is a hot topic in the Machine Learning and Computer Vision community. 
There are different approaches to handle different situations: Supervised learning methods depend on the availability of labeled training data to calculate e.g.\ margins and make them as big as possible, whereas unsupervised methods aim at making sense of the data without any examples. 
Vilari\~no et al.\ used gaze observations to directly generate training examples for the widely used Support Vector Machine (SVM) classifier to show some first promising results. 
This classifier optimizes a linear\footnote{Nonlinear decision boundaries can be obtained by using kernels which give the SVM classifier a cheap possibility to compute a linear boundary in a nonlinearly deformed space.} boundary between the classes of a given training set by maximizing the margins of the training samples to the decision boundary. Instead of directly using the gaze positions as training set for a fully supervised method like a SVM classifier, we investigated in this project the meta-problem of creating correct training data from gaze positions for later usage in supervised settings. 
We formulate this meta-problem itself as a classification-/segmentation task and put it into the context of a semi-supervised setting. 
We are going to use a standard Gradient Boosting algorithm to optimize a loss function that will be formulated in an intuitive way, taking into consideration the fact that for a big part of the given data, we don't know the labels.

The idea of boosting is that while a simple ``weak'' classifier might produce predictions that are only slightly better than random guessing, combining several weak classifiers could produce a powerful committee. 
Boosting sequentially applies the weak classifier (e.g.\ decision trees or neural nets) to modified versions of the original input data. 
The modification of the input data in each step depends on the previously generated classifier: Observations that were misclassified by the previous classifier get higher weights and the weights for already correctly classified observations are decreased. More specifically, the new weights depend on the margins that were obtained in the previous iterations. 
A very popular boosting algorithm is the so-called AdaBoost algorithm introduced by Freund and Schapire in 1997 (\cite{freund1997decision}). 
From a statistical point of view, this algorithm minimizes the exponential loss of the margin, a loss function that gives a very high penalty to negative margins (i.e.\ wrongly classified samples),$\sum_i e^{-y_i f(x_i)}$. 
It turned out that this is not always the desired thing to do; especially if the training data contain outliers, concentrating on the correct classification of those leads to a bad generalizatin error and it might be better to tolerate the outlier to be wrongly classified. 
That AdaBoost can be interpreted as an optimization algorithm (more precisely, forward stagewise additive modeling) based on exponential loss was discovered only later, and with it the desire to develop simple feasible boosting algorithms for arbitrary loss functions. 
However, for arbitrary (convex, differentiable) loss functions it is not trivial to solve the optimization problem arising in each step of the mentioned forward stagewise additive modeling approach: 
In each step, what has to be found are the parameters for the optimal weak classifier (e.g.\ a decision tree) that minimizes the loss function, if added to the current model. 
For exponential loss, this simplifies to a weighted exponential criterion for the new tree and a greedy recursive-partitioning algorithm can be used with this weighted exponential loss as a splitting criterion. 

On the other hand, finding the unconstrained minimum of an arbitrary differentiable convex function could be done via numerical optimization methods such as steepest gradient descent. 
However, in our setting, the therefore needed gradients are defined only on the training data points, and we have the constraint that tree-components, unlike the negative gradient components used in plain steepest descent, have to be predictions of a decision tree instead of the unconstrained maximal descent direction. 
A way to get a predictive model (i.e.\ a model that generalizes to previously unseen data, which is the ultimate goal) is, to induce a basic classifier (decision tree) at each iteration that is fit to the calculated gradients. This ``modification'' of a steepest decent method leads to the so-called gradient boosting algorithm that allows optimizing arbitrary convex loss functions.
For more detailed explanations of this algorithm, refer to \cite[Chapter~10]{friedman2009elements}.

\section{Build a classifier using gaze-tracking}
In 2007, Vilari\~no et al. published a method that used gaze-tracking for polyp detection (\cite{vilarino2007automatic}). 
The core idea of this paper was to train a classifier using expert's gaze positions to automatically generate training samples, instead of manually labeled training data. 
They chose a very simple approach to achieve this and assumed for each video frame that the gaze position indicates a true positive image region and therefore declared a $128\times128$-pixels image patch as their positive sample. All the other non-overlapping regions of the frames were declared as negative samples.
The availability of labeled training data is the bottleneck for enabling Machine Learning and Computer Vision methods for clinical applications. 
The idea of Vilari\~no et al. has a great potential to tackle this issue because it is aimed at reducing the time costs to create training data for classifiers. 
The results reported in \cite{vilarino2007automatic} were, even if not yet suitable for clinical application, promising (see performance plots in Figure \ref{fig:vilarino-results}).
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{vilarino_results_ROC-PR}
	\caption{reported results in \cite{vilarino2007automatic} (AUC = 0.93)}.
	\label{fig:vilarino-results}
\end{figure}

However, the application of Vilari\~no et al.\ was limited to polyp detection in colonoscopy videos and especially, their way to build the training set from gaze observations implicitely used the following assumptions:
\begin{enumerate}
 \item in each video frame, there was at most one structure of interest (polyp)
 \item a structure of interest never exceeded the size of $128 \times 128$ pixels
\end{enumerate}
In their case, this might be justified (examples of how their data looked like are shown in Figure \ref{fig:vilarinoPolypExamples}). 
In fact, their evaluation is not meaningful as soon as one of the above assumptions is not fulfilled. What is evaluated is simply a measure of how well an SVM classifier could separate observed from non-observed image patches. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{vilarino-polyp-examples}
	\caption{some examples of image patches containing polyps in Vilari\~no et al.'s work. The pictures are taken from \cite{vilarino2007automatic}}.
	\label{fig:vilarinoPolypExamples}
\end{figure}

Using datasets that fulfill assumptions 1 and 2 from above, we could achieve visually reasonable results regarding detection with Vilari\~no's approach (see Figure \ref{fig:theirapproachairplane}). 
As we are aiming for a pixel-/voxelwise segmentation of our input, we have to capture the object boundaries and regarding this task, we could improve the quality of the output by using color-based features of SLIC superpixels instead of the values of pre-processed image patches to describe image regions. 
Some example outputs are shown in Figure \ref{fig:airplaneSLIC}. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00189}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 189)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame189-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00249}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 249)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame249-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}	
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame249-svm-patches-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}	
	\caption{inputs and outputs obtained with Vilari\~no's approach. For the shown examples, we used a SVM classifier ({\tt libsvm} package for MATLAB) with a RBF kernel ($\gamma = 0.625$) and a rather high regularization value of $c = 10$. Note that the region containing the airplane was at least partly detected.}
	\label{fig:theirapproachairplane}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00189}
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 189)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-input-frame_00249}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{input (frame 249)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-binaryOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{binary output}
	\end{subfigure}	
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{airplane-heatmapOutput-frame189-svm-superpixelsColor-c10}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{heat map}
	\end{subfigure}	
	\caption{inputs and outputs obtained with color-based features of SLIC superpixels. We used the same classifier as in Figure \ref{fig:theirapproachairplane}, but used only a subset of the negatively labeled superpixels for training, as the SVM could not handle the class imbalance and assigned negative labels to all the results).}
	\label{fig:airplaneSLIC}
\end{figure}

However, there are countless cases where the assumptions are not fulfilled and those datasets cause conceptual and practical problems. 
In the ideal case the gaze observations give us one true positive location for each frame. 
However, the assumption that all the other non-overlapping patches / superpixels are negative, is not valid, if one of the above-mentioned assumptions is not fulfilled: 
Figure \ref{fig:nonValidAssumptionD2} shows an example where assumption 2 fails: The object to be segmented has a large extent and it is problematic to use the gaze observations in the way it was done in \cite{vilarino2007automatic} to generate positive and negative ground truth labels for training. It is clearly visible that the negative training patches / superpixels are not necessarily true negatives.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset2gazePositionFrame207}
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{frame 207 (red: gaze position)}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	    \includegraphics[width=\textwidth]{dataset2SLICsegmentationFrame207}
	    %\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
	    \caption*{SLIC superpixels (red: gaze position)}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{dataset2positivePatchFrame207}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{positive patch}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.31\textwidth}
		\includegraphics[width=\textwidth]{dataset2positiveSuperpixelFrame207}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{positive superpixel}
	\end{subfigure}
	
	\vspace{3mm}
		\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset2negativePatchesFrame207}	
		%\missingfigure[figwidth=\textwidth]{airplane video input (several frames)}
		\caption*{``negative'' patches}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset2negativeSuperpixelsFrame207}	
		%\missingfigure[figwidth=\textwidth]{results (patch vs. superpixels}
		\caption*{``negative'' superpixels}
	\end{subfigure}	
	\caption{This is one example of a dataset where the contained object clearly has a larger extent than what superpixels / $128 \times 128$-image patches could describe with one observed gaze location.}
	\label{fig:nonValidAssumptionD2}
\end{figure}

Examples of assumption 1 failing are our used datasets ``eye tumor'' and ``cochlea.'' Even though the structures are not big in extent, there are multiple interesting parts in one frame -- in the shown images, all the parts lie (mainly) within the $(128\times128)$-patch and therefore considering all the others patches as negative is not problematic. 
However, it is not clear that this stays like this during the whole sequence. 
In the cochlea example it would be enough for the observer to focus on the lower left part to include positive parts in the negative set. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset7positivePatchFrame47}
		\caption*{eye tumor: frame 47 (red: gaze position with surrounding ($128\times128$)-patch}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	    \includegraphics[width=\textwidth]{dataset7positivePatchFrame47gt}
	    \caption*{ground truth (white: positive) \newline}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset8positivePatchFrame189}	
		\caption*{cochlea: frame 189}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\textwidth]{dataset8positivePatchFrame189gt}	
		\caption*{ground truth}
	\end{subfigure}
	\caption{Here, the structures of interest have more than one part -- whereas they happen to be (almost) contained withing the patch, this might not stay like this during the whole sequence. If parts were only slightly further away from each other, or if the observer's focus were a bit different, we would include quite some positive amount of data in the negative training samples.}
	\label{fig:nonValidAssumptionD78}
\end{figure}

The basic problem is therefore that we can observe only one position at a time, and if there happen to be other positive parts in this very frame, we are forced to ignore it. 
It could be argued that whatever part is not focused in one frame might be focused in a later one. 
Yet, in this case we would already have included this part (although perhaps slightly different looking as in a different frame) in the negative set, possibly multiple times. 
This means that our ``negative'' training set is actually a mixture of positives and negatives.
A natural step is therefore to formulate the problem in a different way, namely not as ``separate observed (positive) from unobserved (negative) samples'', but instead as ``given some positive samples, figure out whether or not the other, unlabeled samples, belong to the positive or to the negative class.''
This so-called PU-problem recently got attention and applications in document classification (\cite{li2003learning}), time series classification (\cite{nguyen2011positive}) and bioinformatics (\cite{elkan2008learning}, \cite{yang2012positive}, \cite{yang2014ensemble}, \cite{yousef2015novel}). 
Most of these methods proceed in two steps: 
\begin{enumerate}
 \item Find reliable negative samples from the unlabeled set.
 \item Use a standard classifier to separate positives and negatives.
\end{enumerate}

Our approach is in that sense different that we will not only include reliable negative samples from the unlabeled set (possibly more and more by applying the above steps iteratively), but instead we include also samples with uncertain labels, using a notion of confidence / certainty. 
We designed an according loss function to use in a standard gradient boosting framework, as further discussed in Chapter \ref{chap:learning-with-unlabeled-data}.

\section{Previous work}
\subsection{Interactive image segmentation}
Boykov et al.\ published in 2006 a very important work about interactive image segmentation which comes fairly close to the problem we have to solve (\cite{boykov2006graph}). 
The approach needs as input an image and user strokes indicating background as well as foreground regions. 
A segmentation is achieved by optimizing a function based on estimated conditional probabilities from the user-provided seeds. 
The quality of the obtained results is very convincing and even 3D objects can be segmented very well in medical images from only a few seeds in one depth slice (see e.g.\ Figure \ref{fig:boykovbones}). 
Yet, there are examples where correcting seeds have to be added in later frames (see \cite[Section~Experimental Results]{boykov2006graph}). 
Using gaze observations we try to obtain piecewise continuous user strokes in three dimensions that would make such corrections unnecessary. However, gaining ``background'' user strokes is not straight forward in this setup and we therefore aimed towards a solution that does not need any negative strokes but instead uses the fact that the positive stroke is available over time / depth.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{boykovbones}
	\caption{A 3D segmentation result obtained by Boykov et al. in \cite{boykov2006graph} from only the shown user strokes from one frame.}.
	\label{fig:boykovbones}
\end{figure}

\subsection{PU-learning, Semi-supervised learning}
The problem of learning from labeled and unlabeled data has been widely investigated in the past decades. 
\todoWriteMore{gradient boost with noise, pu-problem as special case, maybe semi-supervised grad boost,
maybe plessis et al.}