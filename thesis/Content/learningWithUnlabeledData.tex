\chapter{Learning with unlabeled data}
\label{chap:learning-with-unlabeled-data}
In this chapter, we will discuss the inherently noisy labels in the setup of Vilari\~no et al. and how to overcome the issues. 
The underlying conceptual problem to solve is the following: Assuming we have only true positive gaze positions (see Chapter \ref{chap:characterizing-gaze} for a discussion about this assumption), we can still not infer any reliable information about the negative labels. Instead, we are facing the so-called PU-Learning Problem that will be explained in the following.

\section{Problem formulation}
The problem of learning a classifier from positive and unlabeled data is aimed at assigning labels to the unlabeled dataset. 
It can be considered a semi-supervised learning setup: Instead of having a positive and a negative set of examples, we are given an incomplete set of positives and a set of unlabeled examples. 
The unlabeled data contains positive and negative examples which we aim to assign to either the positive or negative class. \todoWriteMore{perhaps mention papers: elkan2008learning, plessis2015convex}
%Plessis et al. \todoRef{reference} found that solving the PU-problem corresponds to minimizing a non-convex loss function, and in 2000? \todoRef{year, reference} they presented a way to formulate it in a convex way. As discussed in Chapter ?? \todoRef{reference}, gradient boosting gives us a general way to minimize a loss function. Following the suggestion of ...? \todoRef{ref to cvx pu-formulation}, we chose the following loss function:
Usually, gradient boosting is used to minimize a loss function $L(y,f(x))$, where $y \in \{-1,1\}^m$ is a vector of labels and $x \in \mathbb{R}^{m\times n}$ is a matrix containing $n$-dimensional features. In our case of the PU-learning problem, however, not all the labels $y_i, i \in \{1,\dots,m\}$ are given. 
Instead we have only an incomplete set of positive labels ($+1$) and the rest is unknown. In order to handle this problem within the gradient boosting framework, we need some pseudo labels (see ???). Our pseudo labels are based on the probability $p_i$ of an unlabeled training sample $x_i$ to be positive. For example, $1 \geq p_i > 0.5$ means that $x_i$ is likely to be positive and $0.5 < p_i \leq 1$ means that $x_i$ is likely to be negative and the pseudo labels are chosen accordingly as
\begin{equation*}
 y_i = 
    \begin{cases}
	+1, \quad & \text{if } p_i \geq 0.5 \\
	-1, \quad & \text{if } p_i < 0.5
      \end{cases}
\end{equation*}
and we can easily convert the probability $p_i$ to a probability $\tilde p_i$ of having chosen the correct pseudo label. 
The loss function is then defined as
\begin{equation*}
L(y,f(x)) = \underbrace{\sum_{\{i :~ y_i = 1\}} e^{-y_i f(x_i)}}_{P} \quad + \quad \gamma\underbrace{\sum_{\{i:~ y_i \text{unknown}\}} \left( \tilde p_i e^{-y_i f(x_i)} + (1-\tilde p_i) e^{y_i f(x_i)}\right)}_{U}, 
\end{equation*}
where $y_i$ denotes the (pseudo-)label of sample $x_i$, $\tilde p_i$ is the confidence that the pseudo-label $y_i$ is correct and $f(x_i)$ is the predicted score of the classifier.
Note that if we consider the fully supervised case, the U-term will be 0 and a standard exponential loss function will be optimized. 
For unlabeled data samples, the U-term of the loss function will heavily penalize negative margins, if we are very confident about our pseudo-label being correct ($\tilde p_i \approx 1$) and it will penalize positive margins, if the pseudo-label is very unlikely to be correct ($\tilde p_i \approx 0$). 
Note that this extreme case can by definition of $y_i$ and $\tilde p_i$ not occur and is therefore just of explanatory value. 
If we do not know whether or not our label is correct or incorrect ($\tilde p_i = 0.5$), then what will be penalized are negative as well as positive margins, as this would mean a confident decision towards one direction based on a randomly chosen pseudo-label (in our case $y_i = 1$, if $\tilde p_i=0.5$). Figure \ref{fig:ourlossfunctionplot} shows the U-term of the loss function for different values of the probability $\tilde p_i$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{loss_function_different_p.pdf}	
  \caption{U-term of the loss function for different values of $p$. For samples whose label is most likely incorrect ($p \approx 0$), small margins mean correct decisions (i.e.\ different from the label) and they are therefore rewarded whereas large margins are penalized. In the case of $p \approx 1$, it is the opposite.}
  \label{fig:ourlossfunctionplot}
\end{figure}

The derivative of the loss function with respect to the classifier's scores is then given by 

\begin{equation*}
 \frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} = 
    \begin{cases}
	-y_i e^{-y_i f(x_i)}, & \text{if $y_i = 1$}\\
	-\gamma \cdot \left(y_i \tilde p_i e^{-y_i f(x_i)} - y_i (1 - \tilde p_i) e^{y_i f(x_i)} \right), & \text{if $y_i$ unknown.}
      \end{cases}
\end{equation*}
Clearly, the key for this method is the probability $p_i$ for each sample $x_i$ to be a positive sample. 

\section{Synthetic data}
We conducted some basic experiments on synthetic data. 
The total size of our synthetic training set contained 160 samples, 80 of which were positive and 80 negative. 
The positive training samples were equally generated from normal distributions $\mathcal{N}(\mu_1,\Sigma_1)$ and $\mathcal{N}(\mu_2, \Sigma_2)$ with 
$$\mu_1= \begin{bmatrix}2 \\ 3 \end{bmatrix}, \quad \Sigma_1 = \begin{bmatrix}0.7 & 0.2 \\ 0.2 & 0.5 \end{bmatrix}, \qquad \mu_2 = \begin{bmatrix}4.5 \\ 2 \end{bmatrix}, \quad \Sigma_2 = \begin{bmatrix} 0.2 & 0 \\ 0 & 0.2 \end{bmatrix}$$
and the 80 negative samples were generated from a normal distribution with parameters 
$$\mu_3 = \begin{bmatrix} 2\\1.5\end{bmatrix}, \quad \Sigma_3 = \begin{bmatrix}0.6 & 0.1\\ 0.1 & 0.7\end{bmatrix}$$
as visualized in Figure \ref{fig:synthetic_train_data}. The test set consists of 1000 samples that are identically distributed as the training set.
\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{synthetic-gaussians-contour.pdf}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{synthetic-gaussians-surf.pdf}	
	\end{subfigure}
	\caption{The used distributions}
	\label{fig:synthetic-gaussians}
\end{figure}

As a reference, we optimized a standard exponential loss using all the labels from the training set with a gradient boosting method with decision tree stumps as weak learners and a shrinkage factor of $0.1.$ 
The other experiments were done with the same algorithm, but different assumptions about the available input labels. 
First, we simulated the case of separating ``observed'' from ``unobserved'' samples, i.e.\ we tried to separate a few positive samples from all the other samples. 
In the real setting, this corresponds to the naive approach of separating the image regions that were hit by the user's gaze from all the other regions. 
As expected, there is a considerable performance loss when using this approach (see Figure \ref{fig:synthetic_results}). 
Our second experiment shows the performance achieved with the standard exponential loss using 5 known positives and some pseudo-labels for the other samples. 
The pseudo-labels were assigned according to the probabilities of the known underlying distributions (see Figure \ref{subfig:pu_train}). 
To test the performance of our PU-loss function, we used the same 5 known positives and the probabilities for the unknown labels were given as they were used before to find the pseudo-labels. 
Finally, we followed the suggestion of Du Plessis et al.\ (\cite{plessis2015convex}) and used the double hinge loss and its composite as shortly described in chapter \ref{chap:background} with the same 5 known positives and the same way of assigning pseudo-labels.
Our loss-function outperforms the reference approach using the true training labels as well as the standard exponential loss with the ``correct'' pseudo-labels (that is, the pseudo-labels according to the underlying distributions). 
This can be explained with the fact that our loss function actually takes into account the confidence about the chosen pseudo-labels and adjusts the penalties accordingly. 
Optimizing the double hinge loss and its composite as suggested in \cite{plessis2015convex} yielded better results than separating observed from unobserved data points, but could not outperform our loss-function. 

\todoWriteMore{put somewhere: Du Plessis et al. showed in \cite{plessis2014PUanalysis} that solving the PU-problem corresponds to minimizing a non-convex loss function, and recently they presented a convex formulation by using different loss functions for the positive and the unlabeled data samples in \cite{plessis2015convex}. As discussed in Chapter \ref{chap:background}, gradient boosting gives us a general way to minimize a loss function, and we also tried their suggested double hinge loss $l(z) = \max(-z,\max(0,0.5 - 0.5 z))$ (for unlabeled samples) and its composite $\tilde l(z) = l(z) - l(-z) = -z$ (for positive samples). Whereas du Plessis et al.\ construct an empirical version of the objective function that can be optimized using quadratic programming, we tried to solve the problem using gradient boosting techniques -- in this setup, what is usually done is an estimation of the labels for the unlabeled set and then, based on this, a supervised learning process. 
There have been suggestions to adaptively set the labels for the unlabeled data (ref: SSMBoost [d'Alch\`e-Buc et al., 2002], SemiBoost [Mallapragada et al., 2009]). }
\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_train_data.pdf}	
		\caption{training data (160 samples generated from the described distributions)\newline}
		\label{subfig:ref_train}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_pu_train_data.pdf}	
		\caption{input for the PU-loss: 5 known positives (blue), pseudo-labels (red = -1 / green = +1), probability weights $\tilde p_i$ that pseudo-labels are correct (circle radius)}
		\label{subfig:pu_train}
	\end{subfigure}
	\caption{\subref{subfig:ref_train}) training data for the standard approach \subref{subfig:pu_train}) known positives, pseudo-labels and weights used with the standard exploss and the pu-loss, respectively}
	\label{fig:synthetic_train_data}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_results_roc.pdf}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_results_pr.pdf}	
	\end{subfigure}
	\caption{Comparisons of results using different assumptions about available labels. The PU-loss outperforms the standard exploss, even when using the theoretically correct pseudo-labels.}
	\label{fig:synthetic_results}
\end{figure}


\section{On the real data}
\subsection{labels inferred from ground truth}
Using ground truth data for the three datasets ``instrument'', ``eye tumor'' and ``cochlea,'' we checked on real data how well the different approaches with different assumptions about available labels perform. 
Assuming that we have for a certain amount of frames / depth slices the correct superpixel labels \footnote{$y_i = 1$, if more than 50\% of the pixels contained in a superpixel are positive in the ground truth}, we again optimized a standard exponential loss with a gradient boosting algorithm using decision tree stumps as weak learners. 
We evaluated how well the classifier generalized to the remaining frames, on a pixelwise basis; that is, we classify whole superpixels, but at the end we are interested in the pixelwise segmentation result and compute Receiver Operator Characteristics (ROC) and Precision-Recall values on a pixelwise basis (Figure \ref{fig:reference-known-labels}). This strategy allows us to compare the results to the real pixelwise ground truth instead of a ``constructed'' ground truth of e.g.\ declaring a superpixel as positive, if it contains more than 30\% or 50\% positive ground truth pixels.
Especially if superpixels contain positive and negative ground truth regions, these scores might look a bit odd due to ambiguities in sorting of the resulting scores (all pixels of one superpixel get the same score), but they still give a good idea about the actual performance of the different methods. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{reference_ROC.pdf}	
		\caption*{ROC curves}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{reference_PR.pdf}	
		\caption*{PR curves}
	\end{subfigure}
	\caption{Reference curves. Blue: instrument; red: eye tumor; yellow: cochlea. Note that the precision-recall curves for indicate that the obtained classifier is most confident about positive decisions that are actually wrong. This might indicate that our features are not very suitable for the task, and it is also a hint to the fact that very similar structures appear in the images that are actually sometimes positive and sometimes negative. A further reason is that only a few frames of those datasets contain positive information at all. As the user presses the key only if he sees ``interesting'' (positive) information, we gathered the positive and negative labels from the frames with a pressed key. Therefore, the positive information might be over-represented whereas at the same time some significant negative examples (from frames that do not contain any positive information) might not be contained in the training set.}
	\label{fig:reference-known-labels}
\end{figure}

\subsection{Labels inferred from ground truth}
In our setting, the best we can hope for is one true positive superpixel in every frame. Choosing one true positive superpixel at random from the observed frames\footnote{the frames where the key was pressed in one of the gaze position observations}, and setting the others to negative / unlabeled, we can see in Figure \ref{fig:one-random-tp-per-frame} that our loss function leads to comparable results to a standard gradient boosting classifier, if we find a reasonable way to estimate the probabilities \todo{explain how this was done} of unlabeled superpixels being positive or negative. 
However, the standard classifier will strictly try to separate observed from non-observed superpixels, which is only the correct behaviour if the assumptions discussed in Chapter \ref{chap:background} are fulfilled -- if they are not fulfilled, it means that the learned classifier holds a bias (this has been discussed e.g.\ in \todoRef{give reference}). 
That this is truly happening in this case can, for example, be seen from the heat maps in Figure \ref{fig:bias-in-heatmaps}: Using our constructed PU-loss function, 0 becomes a reasonable threshold to distinguish between positive and negative samples, whereas for the other classifiers we would have to find a threshold below zero to get any positive labels in the output.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{roc-one_random_tp_per_frame.pdf}	
		\caption*{ROC curves}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{pr-one_random_tp_per_frame.pdf}	
		\caption*{PR curves}
	\end{subfigure}
	\caption{Randomly chosen true positive for each frame that was observed (i.e.\ where the user pressed the key) in one of the gaze observations.}
	\label{fig:one-random-tp-per-frame}
\end{figure}


\begin{figure}[ht]
	\centering
 	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-input}
	  \caption*{input image \\ (frame 455 of dataset ``instrument'')}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-groundtruth}
	  \caption*{ground truth \\ \quad}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.495\textwidth}
	  \includegraphics[height=4.4cm]{d2-frame_00455-gradboost-heatmap}
	  \includegraphics[height=4.4cm]{d2-frame_00455-gradboost-colorbar}
	  \caption*{exponential loss \\ (gradient boosting)}
	  % shrinkage = 0.1, 5000 iterations, max tree depth = 2
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.495\textwidth}
	  \includegraphics[height=4.4cm]{d2-frame_00455-svm-heatmap}
	  \includegraphics[height=4.4cm]{d2-frame_00455-svm-colorbar}
	  \caption*{SVM classifier \\ (rbf kernel with $\gamma = 0.0625$, $c = 10$)\\}
	\end{subfigure}	
	\begin{subfigure}[h]{0.495\textwidth}
	  \includegraphics[height=4.4cm]{d2-frame_00455-pugradboost-heatmap}
	  \includegraphics[height=4.4cm]{d2-frame_00455-pugradboost-colorbar}
		  \caption*{PU-losss \\ (gradient boosting)}
		  % 500 iterations, shrinkage = 0.1, stumps
	\end{subfigure}		
	\caption{}
	\label{fig:bias-in-heatmaps}
\end{figure}

\subsection{Labels inferred from gaze observations}
The difference between this best case to our actual situation lies in the distribution of the positive labels -- they are not, as assumed until now, randomly taken from all the positives, but instead they might often over-represent certain positives and under-represent others due to e.g.\ the fact that it is easier to fixate an edge than a smooth region in an image. 
In this case, the naive approach performs clearly worse than ours because ... \todo{... stronger emphasis on whatever samples?} Experiments already done:
\begin{itemize}
\item dataset 2
  \begin{itemize}
  \item standard gradient boost with superpixels (color features) (\path{results/Dataset2/grad_boost_color_gaze2/})
  \item standard gradient boost with patches (preprocessed values as in V. et al.) (\path{results/Dataset2/grad_boost_patches_gaze2/})
  \item pu-gradient boost with superpixels (color features) (\path{results/Dataset2/pu_grad_boost_color_gaze2/})
  \item svm with superpixels (color features) (\path{results/Dataset2/svm_color_gaze2})
  \end{itemize}
\item dataset 7
  \begin{itemize}
  \item ?
  \end{itemize}
\item dataset 8
  \begin{itemize}
  \item ?
  \end{itemize}


\end{itemize}


\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-gaze2-ROCresults-different-loss-functions.pdf}	
		\caption*{ROC curves}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-gaze2-PRresults-different-loss-functions.pdf}	
		\caption*{PR curves}
	\end{subfigure}
	\caption{Results for instrument dataset.}
	\label{fig:d2-results-curves}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-gaze4-ROCresults-different-loss-functions.pdf}	
		\caption*{ROC curves}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-gaze4-PRresults-different-loss-functions.pdf}	
		\caption*{PR curves}
	\end{subfigure}
	\caption{Results for eye tumor dataset.}
	\label{fig:d7-results-curves}
\end{figure}




%mainly because it means that a superpixel that has been labeled positive will have a multitude of negatively labeled ``opponents'' that look almost the same. It can be clearly seen that positions that have been observed only for a short time, even though they belong to the positive set, are not well separated from the negative set because very similar parts will end up in the negative set.



\todoWriteMore{show results with ``normal'' plus/minus 1 labeled data as in Vilari\~no et al., consider patches and also superpixels (good features!)}
\todoWriteMore{contrast to what they did in 1st paper. Training labels according to real ground truth vs. training labels according to gaze position as suggested by Vilari\~no et al.:
...}


