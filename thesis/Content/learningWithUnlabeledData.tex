\chapter{Learning with unlabeled data}
\label{chap:learning-with-unlabeled-data}
In this chapter, we will discuss the inherently noisy labels in the setup of Vilari\~no et al. and how to overcome the issues. 
The underlying conceptual problem to solve is the following: Assuming we have only true positive gaze positions (see Chapter \ref{chap:characterizing-gaze} for a discussion about this assumption), we can still not infer any reliable information about the negative labels. Instead, we are facing the so-called PU-Learning Problem that will be explained in the following.

\section{Problem formulation}
The problem of learning a classifier from positive and unlabeled data is aimed at assigning labels to the unlabeled dataset. 
It can be considered a semi-supervised learning setup: Instead of having a positive and a negative set of examples, we are given an incomplete set of positives and a set of unlabeled examples. 
The unlabeled data contains positive and negative examples which we aim to assign to either the positive or negative class. 
Usually, gradient boosting is used to minimize a loss function $L(y,f(x))$, where $y \in \{-1,1\}^m$ is a vector of labels and $x \in \mathbb{R}^{m\times n}$ is a matrix containing $n$-dimensional features. In our case of the PU-learning problem, however, not all the labels $y_i, i \in \{1,\dots,m\}$ are given. 
Instead we have only an incomplete set of positive labels ($+1$) and the rest is unknown. In order to handle this problem within the gradient boosting framework, we need some pseudo labels (see ???). Our pseudo-labels are based on the probability $p_i$ of an unlabeled training sample $x_i$ to be positive:
\begin{equation*}
 y_i = 
    \begin{cases}
	+1, \quad & \text{if } p_i \geq 0.5, \\
	-1, \quad & \text{if } p_i < 0.5.
      \end{cases}
\end{equation*}
We can easily convert the probability $p_i$ to a probability $\tilde p_i$ of having chosen the correct pseudo-label:
\begin{equation*}
 \tilde p_i = 
    \begin{cases}
	p_i, \quad & \text{if } p_i \geq 0.5, \\
	1-p_i, \quad & \text{if } p_i < 0.5.
      \end{cases}
\end{equation*}

The loss function is then defined as
\begin{equation*}
L(y,f(x)) = \underbrace{\sum_{\{i :~ y_i = 1\}} e^{-y_i f(x_i)}}_{P} \quad + \quad \gamma\underbrace{\sum_{\{i:~ y_i \text{unknown}\}} \left( \tilde p_i e^{-y_i f(x_i)} + (1-\tilde p_i) e^{y_i f(x_i)}\right)}_{U}, 
\end{equation*}
where $y_i$ denotes the (pseudo-)label\footnote{The notation does not distinguish between real labels and pseudo-labels. For the part of the sum with unknown labels, $y_i$ denotes the pseudo-labels.} of sample $x_i$, $\tilde p_i$ is the confidence that the pseudo-label $y_i$ is correct,$f(x_i)$ is the predicted score of the classifier and $\gamma$ is a weight for the U-term that we set to be the relative amount of unlabeled samples in the dataset, $\gamma = \frac{n_{\text{unlabeled}}}{n_{\text{total}}}$ for our experiments. 
Note that if we consider the fully supervised case, the U-term will be 0 and a standard exponential loss function will be optimized. 
For unlabeled data samples, the U-term of the loss function will heavily penalize negative margins, if we are very confident about our pseudo-label being correct ($\tilde p_i \approx 1$) and it will penalize positive margins, if the pseudo-label is very unlikely to be correct ($\tilde p_i \approx 0$). 
Note that this extreme case can by definition of $y_i$ and $\tilde p_i$ not occur and is therefore just of explanatory value. 
If we do not know whether or not our label is correct or incorrect ($\tilde p_i = 0.5$), then what will be penalized are negative as well as positive margins, as this would mean a confident decision towards one direction based on a randomly chosen pseudo-label (in our case $y_i = 1$, if $\tilde p_i=0.5$). Figure \ref{fig:ourlossfunctionplot} shows the U-term of the loss function for different values of the probability $\tilde p_i$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{loss_function_different_p.pdf}	
  \caption{U-term of the loss function for different values of $p$. For samples whose label is most likely incorrect ($p \approx 0$), small margins mean correct decisions (i.e.\ different from the label) and they are therefore rewarded whereas large margins are penalized. In the case of $p \approx 1$, it is the opposite.}
  \label{fig:ourlossfunctionplot}
\end{figure}

The derivative of the loss function with respect to the classifier's scores is then given by 

\begin{equation*}
 \frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} = 
    \begin{cases}
	-y_i e^{-y_i f(x_i)}, & \text{if $y_i = 1$}\\
	-\gamma \cdot \left(y_i \tilde p_i e^{-y_i f(x_i)} - y_i (1 - \tilde p_i) e^{y_i f(x_i)} \right), & \text{if $y_i$ unknown.}
      \end{cases}
\end{equation*}
Clearly, the key of this method is the usage of the probability $p_i$ for each sample $x_i$ to be a positive sample. 
It naturally gives us a way to tune the algorithm not to concentrate the same way on all wrongly classified samples, but instead embrace the fact that there is not always a 100\% certainty that we are working with the correct labels.

\section{Synthetic data}
We conducted some basic experiments on synthetic data. 
The total size of our synthetic training set contained 160 samples, 80 of which were positive and 80 negative. 
The positive training samples were equally generated from normal distributions $\mathcal{N}(\mu_1,\Sigma_1)$ and $\mathcal{N}(\mu_2, \Sigma_2)$ with 
$$\mu_1= \begin{bmatrix}2 \\ 3 \end{bmatrix}, \quad \Sigma_1 = \begin{bmatrix}0.7 & 0.2 \\ 0.2 & 0.5 \end{bmatrix}, \qquad \mu_2 = \begin{bmatrix}4.5 \\ 2 \end{bmatrix}, \quad \Sigma_2 = \begin{bmatrix} 0.2 & 0 \\ 0 & 0.2 \end{bmatrix}$$
and the 80 negative samples were generated from a normal distribution with parameters 
$$\mu_3 = \begin{bmatrix} 2\\1.5\end{bmatrix}, \quad \Sigma_3 = \begin{bmatrix}0.6 & 0.1\\ 0.1 & 0.7\end{bmatrix}$$
as visualized in Figure \ref{fig:synthetic_train_data}. The test set consists of 1000 samples that are identically distributed as the training set.
\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{synthetic-gaussians-contour.pdf}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{synthetic-gaussians-surf.pdf}	
	\end{subfigure}
	\caption{The used distributions}
	\label{fig:synthetic-gaussians}
\end{figure}

As a reference, we optimized a standard exponential loss using all the labels from the training set with a gradient boosting method with decision tree stumps as weak learners and a shrinkage factor of $0.1.$ 
The other experiments were done with the same algorithm, but different assumptions about the available input labels. 
First, we simulated the case of separating ``observed'' from ``unobserved'' samples, i.e.\ we tried to separate a few positive samples from all the other samples. 
In the real setting, this corresponds to the naive approach of separating the image regions that were hit by the user's gaze from all the other regions. 
As expected, there is a considerable performance loss when using this approach (see the red curve Figure \ref{fig:synthetic_results}). 
Our second experiment shows the performance achieved with the standard exponential loss using 5 known positives and some pseudo-labels for the other samples. 
The pseudo-labels were assigned according to the probabilities of the known underlying distributions (see Figure \ref{subfig:pu_train}). 
To test the performance of our PU-loss function, we used the same 5 known positives and the probabilities for the unknown labels were given as they were used before to find the pseudo-labels. 
Finally, we followed the suggestion of Du Plessis et al.\ (\cite{plessis2015convex}) and used the double hinge loss and its composite as shortly described in chapter \ref{chap:background} with the same 5 known positives and the same way of assigning pseudo-labels.
Our loss-function outperforms the reference approach using the true training labels as well as the standard exponential loss with the ``correct'' pseudo-labels (that is, the pseudo-labels according to the underlying distributions). 
This can be explained with the fact that our loss function actually takes into account the confidence about the chosen pseudo-labels and adjusts the penalties accordingly. 
Optimizing the double hinge loss and its composite as suggested in \cite{plessis2015convex} yielded better results than separating observed from unobserved data points, but could not outperform our loss-function. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_train_data.pdf}	
		\caption{training data (160 samples generated from the described distributions)\newline}
		\label{subfig:ref_train}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_pu_train_data.pdf}	
		\caption{input for the PU-loss: 5 known positives (blue), pseudo-labels (red = -1 / green = +1), probability weights $\tilde p_i$ that pseudo-labels are correct (circle radius)}
		\label{subfig:pu_train}
	\end{subfigure}
	\caption{\subref{subfig:ref_train}) training data for the standard approach \subref{subfig:pu_train}) known positives, pseudo-labels and weights used with the standard exploss and the pu-loss, respectively}
	\label{fig:synthetic_train_data}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_results_roc.pdf}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_results_pr.pdf}	
	\end{subfigure}
	\caption{Comparisons of results using different assumptions about available labels. The PU-loss outperforms the standard exploss, even when using the theoretically correct pseudo-labels.}
	\label{fig:synthetic_results}
\end{figure}


\section{On the real data}
Our real datasets consist of multiple images each that we pre-segmented using the SLIC algorithm. 
We decided to use some simple descriptors for the superpixels: For the color videos (airplane from Figure \ref{fig:airplaneSLIC} and instrument dataset), we used three-dimensional features
\begin{equation*}
 x = \left( \frac{\bar r}{\bar g + \bar r + 10^{-5}}, \quad \frac{\bar r }{\bar b +\bar r + 10^{-5}}, \quad \frac{\bar g}{ \bar b + \bar g + 10^{-5}}\right),
\end{equation*}
$\bar r, \bar g$ and $\bar b$ being the average of red, green and blue values within one superpixel, respectively. 
For the eye tumor and the cochlea datasets we used the following values to describe a superpixel:
\begin{itemize}
 \item an intensity histogram (10 bins),
 \item average intensity,
 \item variance of intensity,
 \item and the co-occurrence matrix (\cite{haralick1973textural}) of the image when masked to see only the superpixel.
\end{itemize}

\subsection{Labels inferred from ground truth}
Using ground truth data for the three datasets ``instrument'', ``eye tumor'' and ``cochlea,'' we checked how well the different approaches with different assumptions about available labels perform. 
Assuming that we have for a certain amount of frames / depth slices the correct superpixel labels\footnote{$y_i = 1$, if more than 50\% of the pixels contained in a superpixel are positive in the ground truth}, we again optimized a standard exponential loss with a gradient boosting algorithm using decision tree stumps as weak learners. 
This serves as a reference for our experiments.
We evaluate how well the classifier generalizes to the remaining frames on a pixelwise basis; that is, we classify whole superpixels, but at the end we are interested in the pixelwise segmentation result and compute Receiver Operator Characteristics (ROC) and Precision-Recall values on a pixelwise basis. 
For the eye tumor MRI and the cochlea CT scan, one issue with the evaluation was that there are only a few slices containing true positive information, and usually the user pressed the key during most of these frames. 
Therefore, evaluating only the generalization to the remaining frames is not a reasonable approach and, unlike in the instrument dataset, it will not tell us enough about the potential of separating the interesting structure from the rest. 
Instead, in these datasets the evaluation has been done for a big part of the whole volume, including already seen superpixels during training. 
If we are later using only gaze observations, we will have to evaluate as well every superpixel of every frame.
This strategy allows us to compare the results to the real pixelwise ground truth instead of a ``constructed'' ground truth of e.g.\ declaring a superpixel as positive, if it contains more than 30\% or 50\% positive ground truth pixels.
Especially if superpixels contain positive and negative ground truth regions, the resulting curves might look a bit odd due to ambiguities in sorting of the resulting scores (all pixels of one superpixel get the same score), but they still give a good idea about the actual performance of the different methods. 

In our setting, the best we can hope for is one true positive superpixel in every frame. 
Choosing one true positive superpixel at random from some frames\footnote{the frames where the key was pressed in one of the gaze position observations}, and setting the others to negative / unlabeled, we can see in Figure \ref{fig:one-random-tp-per-frame} that our PU-loss function could gain a small performance gain with respect to the measured AUC for all the datasets. \todo{is there a connection to the paper that showed it's only about thresholding???}
However, the standard classifier will strictly try to separate observed from non-observed superpixels, which is only the correct behaviour if the assumptions discussed in Chapter \ref{chap:background} are fulfilled -- if they are not fulfilled, it means that the learned classifier holds a bias (this has been discussed e.g.\ in \todoRef{give reference}). 
That this is truly happening in this case can, for example, be seen from the heat maps in Figure \ref{fig:bias-in-heatmaps}: Using our constructed PU-loss function, 0 becomes a reasonable threshold to distinguish between positive and negative samples, whereas for the other classifiers we would have to find a threshold below zero to get any positive labels in the output. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-one_random_tp_per_frame-roc.pdf}	
		\caption*{instrument}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-one_random_tp_per_frame-pr.pdf}	
		\caption*{instrument}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-one_random_tp_per_frame-roc.pdf}	
		\caption*{eye tumor}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-one_random_tp_per_frame-pr.pdf}	
		\caption*{eye tumor (tested frames: [12:80])}
	\end{subfigure}	
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-one_random_tp_per_frame-roc.pdf}	
		\caption*{cochlea}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-one_random_tp_per_frame-pr.pdf}	
		\caption*{cochlea (tested frames: [70:250])}
	\end{subfigure}		
	\caption{Randomly chosen true positive for each frame that was observed (i.e.\ where the user pressed the key) in one of the gaze observations.}
	\label{fig:one-random-tp-per-frame}
\end{figure}


\begin{figure}[ht]
	\centering
 	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-input}
	  \caption*{input image \\ (frame 455 of dataset ``instrument'')}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-groundtruth}
	  \caption*{ground truth \\ \quad}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.495\textwidth}
	  \includegraphics[height=4.4cm]{d2-frame_00455-gradboost-heatmap}
	  \includegraphics[height=4.4cm]{d2-frame_00455-gradboost-colorbar}
	  \caption*{exponential loss \\ (gradient boosting)}
	  % shrinkage = 0.1, 5000 iterations, max tree depth = 2
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.495\textwidth}
	  \includegraphics[height=4.4cm]{d2-frame_00455-svm-heatmap}
	  \includegraphics[height=4.4cm]{d2-frame_00455-svm-colorbar}
	  \caption*{SVM classifier \\ (rbf kernel with $\gamma = 0.0625$, $c = 10$)\\}
	\end{subfigure}	
	\begin{subfigure}[h]{0.495\textwidth}
	  \includegraphics[height=4.4cm]{d2-frame_00455-pugradboost-heatmap}
	  \includegraphics[height=4.4cm]{d2-frame_00455-pugradboost-colorbar}
		  \caption*{PU-losss \\ (gradient boosting)}
		  % 500 iterations, shrinkage = 0.1, stumps
	\end{subfigure}		
	\caption{}
	\label{fig:bias-in-heatmaps}
\end{figure}

\subsection{Labels inferred from gaze observations}
The difference between the above case to our actual situation lies mainly in the distribution of the positive labels -- they are not, as assumed in the previous section, randomly taken from all the positives of the observed frames, but instead they might often over-represent certain positives and under-represent others due to e.g.\ the fact that it is easier to fixate an edge than a smooth region in an image. 
In this setting it becomes more important that not all the unlabeled samples are considered negatives. 
The red curves in Figure \ref{fig:results-curves} show a rather low performance for this approach in each dataset. 
Our loss function on the otherhand enabled us to condsider other superpixels likely to be positive, even if they were never directly observed by the user. 
The performance plots of our method (yellow curves in Figure \ref{fig:results-curves}) show that we clearly outperform the previously presented idea of Vilari\~no et al.\ to consider only one true positive per frame.
%Another issue that makes the problem harder is the present noise in the gaze observations: As discussed in Chapter \ref{chap:characterizing-gaze}, it is not true that each gaze positions indicates a true positive position in the image.
%The naive approach performs clearly worse than ours because it does not distinguish between ... \todo{... stronger emphasis on whatever samples?} Experiments already done:


\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-gaze2-results-roc.pdf}	
		\caption*{instrument}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-gaze2-results-pr.pdf}	
		\caption*{instrument}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-gaze4-results-roc.pdf}	
		\caption*{eye tumor}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-gaze4-results-pr.pdf}	
		\caption*{eye tumor (tested frames: [12:80])}
	\end{subfigure}	
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-gaze2-results-roc.pdf}	
		\caption*{cochlea}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-gaze2-results-pr.pdf}	
		\caption*{cochlea (tested frames: [70:250])}
	\end{subfigure}		
	\caption{Results using only actual gaze observations.}
	\label{fig:results-curves}
\end{figure}




%mainly because it means that a superpixel that has been labeled positive will have a multitude of negatively labeled ``opponents'' that look almost the same. It can be clearly seen that positions that have been observed only for a short time, even though they belong to the positive set, are not well separated from the negative set because very similar parts will end up in the negative set.



