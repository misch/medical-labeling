\chapter{Learning with Unlabeled Data}
\label{chap:learning-with-unlabeled-data}
In this chapter, we will discuss the inherently noisy labels in the negative training set and how to overcome the issues. 
The underlying problem to solve is the following: Assuming we have only true positive samples gained from gaze positions (see Chapter \ref{chap:characterizing-gaze} for a discussion about this assumption), how can we separate positive from negative samples? We formally define the problem and suggest a way to learn from only positive training samples using gradient boosting. Results on synthetic and real data can be found towards the end of the chapter.

\section{Problem Formulation}
The problem of learning a classifier from positive and unlabeled data is to assign labels to the unlabeled dataset \cite{elkan2008learning}. 
It can be considered a semi-supervised learning setup: Instead of having a positive and a negative set of examples, we are given an incomplete set of positives and a set of unlabeled examples. 
The unlabeled data contains positive and negative examples which we want to assign to either the positive or negative class. 
Usually, gradient boosting is used to minimize a loss function $L(y,f(x))$, where $y \in \{-1,1\}^m$ is a vector of labels and $x \in \mathbb{R}^{m\times n}$ is a matrix containing $n$-dimensional features. In our case of the PU-learning problem, however, not all the labels $y_i, i \in \{1,\dots,m\}$ are given. 
Instead we have only an incomplete set of positive labels ($+1$) and the rest is unknown. In order to handle this problem within a gradient boosting framework, we need some pseudo-labels (see e.g.\ \cite{bennett2002exploiting}, \cite{mallapragada2009semiboost}). We define the pseudo-labels to depend on the probability $p_i$ of an unlabeled training sample $x_i$ to be positive:
\begin{equation*}
 y_i = 
    \begin{cases}
	+1, \quad & \text{if } p_i \geq 0.5, \\
	-1, \quad & \text{if } p_i < 0.5.
      \end{cases}
\end{equation*}
We can easily convert the probability $p_i$ to a probability $\tilde p_i$ of having chosen the correct pseudo-label:
\begin{equation*}
 \tilde p_i = 
    \begin{cases}
	p_i, \quad & \text{if } p_i \geq 0.5, \\
	1-p_i, \quad & \text{if } p_i < 0.5.
      \end{cases}
\end{equation*}

The loss function is then defined as
\begin{equation*}
L(y,f(x)) = \underbrace{\sum_{\{i :~ y_i = 1\}} e^{-y_i f(x_i)}}_{P} \quad + \quad \gamma\underbrace{\sum_{\{i:~ y_i \text{unknown}\}} \left( \tilde p_i e^{-y_i f(x_i)} + (1-\tilde p_i) e^{y_i f(x_i)}\right)}_{U}, 
\end{equation*}
where $y_i$ denotes the (pseudo-)label\footnote{The notation does not distinguish between real labels and pseudo-labels. For the part of the sum with unknown labels, $y_i$ denotes the pseudo-labels.} of sample $x_i$, $\tilde p_i$ is the confidence that the pseudo-label $y_i$ is correct, $f(x_i)$ is the predicted score of the classifier and $\gamma$ is a weight for the U-term that we set to be the relative amount of unlabeled samples in the dataset, $\gamma = n_{\text{unlabeled}}/n_{\text{total}}$ for our experiments. 
In a fully supervised case, the U-term is zero and a standard exponential loss function is optimized. 
For unlabeled data samples, the U-term of the loss function heavily penalizes negative margins, if we are very confident about our pseudo-label ($\tilde p_i \approx 1$) and it will penalize positive margins, if the pseudo-label is very unlikely to be correct ($\tilde p_i \approx 0$). 
Note that this extreme case can by definition of $y_i$ and $\tilde p_i$ not occur and is therefore just of explanatory value. 
The minimum of the loss functions varies and gets closer to zero with a decreasing confidence $\tilde p_i$. If we do not know whether or not the pseudo-label is correct or incorrect ($\tilde p_i = 0.5$), then the minimum is at a margin of exactyl zero and what will be penalized are negative as well as positive margins. For such samples, no decision can be made, as this would mean a decision towards one direction based on a randomly chosen pseudo-label (in our case $y_i = 1$, if $\tilde p_i=0.5$). Figure \ref{fig:ourlossfunctionplot} shows the U-term of the loss function for different values of the probability $\tilde p_i$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{loss_function_different_p.pdf}	
  \caption{U-term of the loss function for different values of $p$. For samples whose label is most likely incorrect ($p \approx 0$), small margins mean correct decisions (i.e.\ different from the label) and they are therefore rewarded whereas large margins are penalized. In the case of $p \approx 1$, it is the opposite.}
  \label{fig:ourlossfunctionplot}
\end{figure}

The derivative of the loss function with respect to the classifier's scores is then given by 

\begin{equation*}
 \frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} = 
    \begin{cases}
	-y_i e^{-y_i f(x_i)}, & \text{if $y_i = 1$}\\
	-\gamma \cdot \left(y_i \tilde p_i e^{-y_i f(x_i)} - y_i (1 - \tilde p_i) e^{y_i f(x_i)} \right), & \text{if $y_i$ unknown.}
      \end{cases}
\end{equation*}

The key of this method is the usage of the probability $p_i$ for each sample $x_i$ to be a positive sample. 
It gives a natural way to tune the algorithm not to concentrate the same way on all wrongly classified samples, but to instead embrace the fact that there is not always a 100\% certainty that we are working with the correct labels.

\section{Synthetic Data}
We conducted some basic experiments on synthetic data. 
The total size of our synthetic training set contained 160 samples, 80 of which were positive and 80 negative. 
The positive training samples were equally generated from normal distributions $\mathcal{N}(\mu_1,\Sigma_1)$ and $\mathcal{N}(\mu_2, \Sigma_2)$ with 
$$\mu_1= \begin{bmatrix}2 \\ 3 \end{bmatrix}, \quad \Sigma_1 = \begin{bmatrix}0.7 & 0.2 \\ 0.2 & 0.5 \end{bmatrix}, \qquad \mu_2 = \begin{bmatrix}4.5 \\ 2 \end{bmatrix}, \quad \Sigma_2 = \begin{bmatrix} 0.2 & 0 \\ 0 & 0.2 \end{bmatrix}$$
and the 80 negative samples were generated from a normal distribution with parameters 
$$\mu_3 = \begin{bmatrix} 2\\1.5\end{bmatrix}, \quad \Sigma_3 = \begin{bmatrix}0.6 & 0.1\\ 0.1 & 0.7\end{bmatrix}$$
as visualized in Figure \ref{fig:synthetic_train_data}. The test set consists of 1000 samples that are identically distributed as the training set.
\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{synthetic-gaussians-contour.pdf}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{synthetic-gaussians-surf.pdf}	
	\end{subfigure}
	\caption{The used distributions to generate the training and test data used for the synthetic experiments. The positive samples were distributed according to the distributions with the means at the top and on the right side in the left Figure, and the negative samples were distributed according to the contours at the lower left.}
	\label{fig:synthetic-gaussians}
\end{figure}

As a reference (blue curves in Figure \ref{fig:synthetic_results}, we optimized a standard exponential loss using all the labels from the training set with a gradient boosting method with decision tree stumps as weak learners and a shrinkage factor of $0.1.$ 
The other experiments were done with the same algorithm, but different assumptions about the available input labels. 
First, we simulated the case of separating observed from unobserved samples, i.e.\ we tried to separate a few positive samples from all the other samples. 
In the real setting, this corresponds to the naive approach used by Vilari\~no et al.\ \cite{vilarino2007automatic} of separating the image regions that were hit by the user's gaze from all the other regions. 
As expected, there is a considerable performance loss when using this approach (see the red curve in Figure \ref{fig:synthetic_results}). 
Our second experiment (yellow curves in Figure \ref{fig:synthetic_results}) shows the performance achieved with the standard exponential loss using 5 known positives and some pseudo-labels for the other samples. 
The pseudo-labels were assigned according to the probabilities of the known underlying distributions (see Figure \ref{subfig:pu_train}). 
To test the performance of our PU-loss function, we used the same 5 known positives and probabilities. 
Our loss-function outperforms the reference approach using the true training labels as well as the standard exponential loss with the ``correct'' pseudo-labels (that is, the pseudo-labels according to the underlying distributions). 
This can be explained with the fact that our loss function actually takes into account the confidence about the chosen pseudo-labels and adjusts the penalties accordingly. 
Optimizing the double hinge loss and its composite as suggested in \cite{plessis2015convex} (green curve) yielded better results than separating observed from unobserved data points, but could not outperform our loss-function. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_train_data.pdf}	
		\caption{Training data (160 samples generated from the distributions illustrated in Figure \ref{fig:synthetic-gaussians})\newline}
		\label{subfig:ref_train}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_pu_train_data.pdf}	
		\caption{Input for the PU-loss: 5 known positives (blue), pseudo-labels (red = -1 / green = +1), probability weights $\tilde p_i$ that pseudo-labels are correct (circle radius)}
		\label{subfig:pu_train}
	\end{subfigure}
	\caption{\subref{subfig:ref_train}) Training data for the standard exponential loss. \subref{subfig:pu_train}) Known positives, pseudo-labels and weights used with the standard exponential loss and the PU-loss, respectively.}
	\label{fig:synthetic_train_data}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_results_roc.pdf}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.49\textwidth}
	\includegraphics[width=\textwidth]{synthetic_results_pr.pdf}	
	\end{subfigure}
	\caption{Comparisons of results using different assumptions about available labels. The PU-loss outperforms the standard exploss, even when using the theoretically correct pseudo-labels.}
	\label{fig:synthetic_results}
\end{figure}


\section{On the Real Data}
\label{sec:real-data}
\subsection{Features}
Each of our real datasets consist of multiple images that we pre-segmented using the SLIC algorithm (\cite{achanta2010slic}, \cite{vedaldi08vlfeat}) with parameters as indicated in Table \ref{tab:slic-params}. 

\begin{table}[ht]
	\centering
	  \caption{Chosen parameters to pre-segment the input images into SLIC superpixels. The color images from the instrument dataset were converted to CIELAB color space before SLIC was applied.}
	  \label{tab:slic-params}
	\begin{tabular}{ | c  c  c | }
	\hline
	  dataset	& region size & regularizer \\ \hline
	  instrument  	& 35 & 300 \\
	  eye tumor  	& 23 & 0.05 \\ 
	  cochlea	& 22 & 0.02 \\ 
	  airplane 	& 29 & 40 \\ \hline
	\end{tabular}
\end{table}

We decided to use simple descriptors for the superpixels; for the color videos (airplane from Figure \ref{fig:airplaneSLIC} and instrument dataset), we used three-dimensional features
\begin{equation*}
 x = \left( \frac{\bar r}{\bar g + \bar r + 10^{-5}}, \quad \frac{\bar r }{\bar b +\bar r + 10^{-5}}, \quad \frac{\bar g}{ \bar b + \bar g + 10^{-5}}\right),
\end{equation*}
$\bar r, \bar g$ and $\bar b$ being the average of red, green and blue values within the superpixel, respectively. 
For the eye tumor and the cochlea datasets we used the following values to describe a superpixel:
\begin{itemize}
 \item an intensity histogram (10 bins),
 \item average intensity,
 \item variance of intensity,
 \item and the co-occurrence matrix (\cite{haralick1973textural}) of the image when masked to see only the superpixel.
\end{itemize}

\subsection{Probabilities}
Using synthetic data, we could profit from knowing the underlying distribution and directly use this knowledge by feeding the true probabilities to our algorithm. 
However, in reality these probabilities are unknown and we have to come up with a way to estimate them. 
The results presented in this thesis were obtained with experimentally found values; given the observed positive superpixels $x_1,\dots,x_m$ from a gaze sequence, we calculated a distance vector $(d_1,\dots,d_m)$ for every unlabeled sample $\tilde x$ with $d_i = d(\tilde x, x_i), i = 1,\dots,m.$
Here, $d$ is a general distance metric. We used euclidean and cosine metrics as indicated in Table \ref{tab:probabilities}. 
The distance vector was then aggregated to one value $d_{\text{f}}$ by taking the median or minimum value. 
Further, we calculated the spatial (euclidean) distance between every unlabeled superpixel and the gaze position in the corresponding frame 
$$d_\text{s} = d\left(\begin{bmatrix} \text{gaze}_x \\ \text{gaze}_y \end{bmatrix}, \begin{bmatrix} x \\ y \end{bmatrix}\right),$$
where $\begin{bmatrix} x \\ y \end{bmatrix}$ is the median position of all the pixels contained in the unlabeled superpixel.
The probabilities were then calculated as indicated in Table \ref{tab:probabilities}.
There is always a spatial term and a feature term involved. The feature term is based on the distance $d_{\text{f}}$ and therefore includes information about all the positives that were collected using the gaze information. The exact parameters and the usage of different metrics was determined experimentally and there is no guarantee that they are the optimal ones.
\begin{table}[ht]
	\centering
	  \caption{A listing of the formulas used for the different datasets to calculate approximate probability values for the unknown labels to be positive.}
	  \label{tab:probabilities}
	\begin{tabular}{ | c  c  p{7cm} | }
	\hline
	  dataset	& formula 								& $d_{\text{f}}$  \\ \hline
			& & \\
	  instrument  	& $p_{\tilde x} = e^{-d_{\text{f}}/0.15} \cdot e^{-d_{\text{s}}/400}$ 	& $d_{\text{f}} = $ median of euclidean distances  \\ 
	  & & \\
	  eye tumor	& $p_{\tilde x} = e^{-d_{\text{f}}/0.15} \cdot e^{-d_{\text{s}}/400}$ 	& $d_{\text{f}} = $ median of cosine distances 	  \\ 
	  & & \\
	  cochlea 	& $p_{\tilde x} = e^{-d_{\text{f}}/0.15k} \cdot e^{-d_{\text{s}}/40}$ 	& $d_{\text{f}} = $ miminum of cosine distances; $k = \max(d_\text{f})$ is a normalization constant used because the minimum cosine distances are small numbers\\ \hline
	\end{tabular}
\end{table}

\subsection{Results}
\subsubsection*{Labels inferred from ground truth}
Using manually labeled ground truth data for the three datasets instrument, eye tumor and cochlea, we explored how well the standard exponential loss and our suggested PU-loss perform with different assumptions about available labels. 
Assuming that we have for a certain amount of frames / depth slices the correct superpixel labels\footnote{$y_i = 1$, if more than 50\% of the pixels contained in a superpixel are positive in the ground truth}, we optimized a standard exponential loss with a gradient boosting algorithm using decision tree stumps as weak learners. 
This serves as a reference for our experiments.
We evaluate how well the classifier generalizes to the remaining frames on a pixelwise basis; that is, we classify whole superpixels, but at the end we are interested in the pixelwise segmentation result and compute Receiver Operator Characteristics (ROC) and Precision-Recall values on a pixelwise basis. 
This strategy allows us to compare the results to the real pixelwise ground truth instead of a constructed ground truth like e.g.\ declaring a superpixel as positive, if it contains more than 30\% or 50\% positive ground truth pixels.
For the eye tumor and the cochlea data there are only a few slices containing true positive information, and usually the user presses and holds the key during most of these frames. 
Therefore, evaluating only the generalization to the remaining frames is not informative and, unlike in the instrument dataset, will not tell us enough about the potential of separating the interesting structure from the rest. 
Instead, in these datasets the evaluation has been done for a the whole volume, including already seen superpixels during training. 
When we are later using only gaze observations, we will not exclude the already seen superpixels either. As our ultimate goal is to achieve a full segmentation we have to evaluate every superpixel of every frame.

In our setting, the best we can hope for is one true positive superpixel in every frame. 
To simulate this case with the ground truth data, we choose one true positive superpixel at random from some frames\footnote{The frames were chosen according to be the ones where the user pressed the key during one of the recorded gaze sequences.}, and set the others to negative / unlabeled. We can see in Figure \ref{fig:one-random-tp-per-frame} that our PU-loss function brings a small performance gain compared to the standard exponential loss (red line) with respect to the measured AUC for all the datasets. 
The standard classifier tries to separate observed from non-observed superpixels. In all our datasets, this is not the correct behaviour and it means that the learned classifier holds a bias (this has been discussed e.g.\ by Du Plessis et al.\ (\cite{plessis2014PUanalysis}) and Figure \ref{fig:bias-in-heatmaps} shows that with the standard exponential loss, we have to find a threshold below zero to get any positive labels in the output. 

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-one_random_tp_per_frame-roc.pdf}	
		\caption*{instrument}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-one_random_tp_per_frame-pr.pdf}	
		\caption*{instrument}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-one_random_tp_per_frame-roc.pdf}	
		\caption*{eye tumor}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-one_random_tp_per_frame-pr.pdf}	
		\caption*{eye tumor (tested frames: [12:80])}
	\end{subfigure}	
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-one_random_tp_per_frame-roc.pdf}	
		\caption*{cochlea}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-one_random_tp_per_frame-pr.pdf}	
		\caption*{cochlea (tested frames: [70:250])}
	\end{subfigure}		
	\caption{Randomly chosen true positive for each frame that was observed (i.e.\ where the user pressed the key) in one of the gaze observations.}
	\label{fig:one-random-tp-per-frame}
\end{figure}


\begin{figure}[ht]
	\centering
 	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-input}
	  \caption*{input image \\ (frame 455 of dataset ``instrument'')}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-groundtruth}
	  \caption*{ground truth \\ \quad}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-reference-heatmap.eps}
	  \caption*{reference: gradient boosting with all the correct positive and negative labels inferred from ground truth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-gradboost-heatmap.eps}
	  \caption*{exponential loss \newline}
	\end{subfigure}	
	~
	\begin{subfigure}[h]{0.48\textwidth}
	  \includegraphics[width=\textwidth]{d2-frame_00455-pugradboost-heatmap.eps}
		  \caption*{PU-loss \\ }
		  % 500 iterations, shrinkage = 0.1, stumps
	\end{subfigure}		
	\caption{Resulting scores for one frame using as input random true positives inferred from the ground truth. Whereas both, the standard exponential loss and our PU-loss yield higher scores for regions containing the object, our approach leads to the threshold separating positives from negatives being closer to 0.}
	\label{fig:bias-in-heatmaps}
\end{figure}

\subsubsection*{Labels inferred from gaze observations}
The difference between the above case to our actual situation lies mainly in the distribution of the positive labels -- they are not, as assumed in the previous section, randomly taken from all the positives of the observed frames, but instead they might often over-represent certain positives and under-represent others due to e.g.\ the fact that it is easier to fixate an edge than a smooth region in an image. 
In this setting it becomes more important that not all the unlabeled samples are considered negatives. 
The red curves in Figure \ref{fig:results-curves} show a rather low performance for this approach in each dataset. 
Our loss function on the otherhand enabled us to condsider other superpixels likely to be positive, even if they were never directly observed by the user. 
The performance plots of our method (yellow curves in Figure \ref{fig:results-curves}) show that we outperform the previously presented idea of Vilari\~no et al.\ to consider only one true positive per frame.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-gaze2-results-roc.pdf}	
		\caption*{instrument}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d2-gaze2-results-pr.pdf}	
		\caption*{instrument}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-gaze4-results-roc.pdf}	
		\caption*{eye tumor}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d7-gaze4-results-pr.pdf}	
		\caption*{eye tumor (tested frames: [12:80])}
	\end{subfigure}	
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-gaze2-results-roc.pdf}	
		\caption*{cochlea}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.45\textwidth}
	\includegraphics[width=\textwidth]{d8-gaze2-results-pr.pdf}	
		\caption*{cochlea (tested frames: [70:250])}
	\end{subfigure}		
	\caption{Results using only actual gaze observations.}
	\label{fig:results-curves}
\end{figure}

Some resulting scores are shown for each dataset in Figures \ref{fig:results-d2-gaze2}, \ref{fig:results-d7-gaze4} and \ref{fig:results-d8-gaze2}. 
The heat maps in Figure \ref{fig:results-d2-gaze2} show that our approach gives e.g.\ higher scores to the tip of the instrument (best visible in frame 738). 
In frame 29, both approaches fail to recognize one part of the tip of the instrument, most likely due to the pre-segmentation that does not respect this particular edge in the image and therefore superpixel contains more negative (background) than positive (instrument) information. Even though the shaft of the instrument did not get positive scores in the optimized PU-loss, it is separated from the surrounding regions, whereas the standard exponential yields non-uniform and rather scores for those superpixels (best visible in frame 29).
In the eye tumor dataset (Figure \ref{fig:results-d7-gaze4}), we see that more superpixels get higher scores whereas the standard exponential loss leads mostly to only one positive per frame (the one that was hit by the gaze) and the others have rather low scores. 
Our approach yields as well the same positives, but shows more uncertainty about the (wrong) negative predictions at the locations of the other true positives. 
The same holds for the example of the cochlea in Figure \ref{fig:results-d8-gaze2}. 
However, we can here as well see that our approach is heavily dependent on the gaze-positions. As elaborated in chapter \ref{chap:characterizing-gaze}, the gaze sequences for the latter two datasets contain less true positive information than for the first dataset. However, in our problem formulation, we assumed that the given extracted superpixels are true positives. For the unlabeled superpixels, the pseudo-labels were estimated based on very noisy information from the extracted positives of the whole sequence. Therefore the final segmentation results for those cases are not satisfying at this stage.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00121-input-segmented}	
		\caption*{frame 121}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00738-input-segmented}	
		\caption*{frame 738}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00029-input-segmented}	
		\caption*{frame 29}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00121-groundtruth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00738-groundtruth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00029-groundtruth}	
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00121-gaze2-gradboost-heatmap.eps}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00738-gaze2-gradboost-heatmap.eps}
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00029-gaze2-gradboost-heatmap.eps}	
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00121-gaze2-pugradboost-heatmap.eps}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00738-gaze2-pugradboost-heatmap.eps}	
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d2-frame_00029-gaze2-pugradboost-heatmap.eps}	
	\end{subfigure}	
	
	\caption{Heat maps showing resulting scores using only actual gaze observations. Top row: Input superpixels. Second row: Ground truth. Third row: resulting scores using standard exponential loss. Bottom row: resulting scores using the PU-loss.}
	\label{fig:results-d2-gaze2}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00043-input-segmented}	
		\caption*{frame 43}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00046-input-segmented}	
		\caption*{frame 46}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00048-input-segmented}	
		\caption*{frame 48}
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00043-groundtruth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00046-groundtruth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00048-groundtruth}	
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00043-gaze4-gradboost-heatmap.eps}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00046-gaze4-gradboost-heatmap.eps}
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00048-gaze4-gradboost-heatmap.eps}	
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00043-gaze4-pugradboost-heatmap.eps}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00046-gaze4-pugradboost-heatmap.eps}	
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d7-frame_00048-gaze4-pugradboost-heatmap.eps}	
	\end{subfigure}	
	
	\caption{Heat maps showing resulting scores using only actual gaze observations. Top row: Input superpixels. Second row: Ground truth. Third row: resulting scores using standard exponential loss. Bottom row: resulting scores using the PU-loss.}
	\label{fig:results-d7-gaze4}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00170-input-segmented}	
		\caption*{frame 170}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00193-input-segmented}	
		\caption*{frame 193}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00197-input-segmented}	
		\caption*{frame 197}
	\end{subfigure}

	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00170-groundtruth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00193-groundtruth}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00197-groundtruth}	
	\end{subfigure}		
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00170-gaze2-gradboost-heatmap.eps}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00193-gaze2-gradboost-heatmap.eps}
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00197-gaze2-gradboost-heatmap.eps}	
	\end{subfigure}
	
	\vspace{3mm}
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00170-gaze2-pugradboost-heatmap.eps}	
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00193-gaze2-pugradboost-heatmap.eps}	
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.32\textwidth}
	\includegraphics[width=\textwidth]{d8-frame_00197-gaze2-pugradboost-heatmap.eps}	
	\end{subfigure}	
	
	\caption{Heat maps showing resulting scores using only actual gaze observations. Top row: Input superpixels. Second row: Ground truth. Third row: resulting scores using standard exponential loss. Bottom row: resulting scores using the PU-loss.}
	\label{fig:results-d8-gaze2}
\end{figure}




%mainly because it means that a superpixel that has been labeled positive will have a multitude of negatively labeled ``opponents'' that look almost the same. It can be clearly seen that positions that have been observed only for a short time, even though they belong to the positive set, are not well separated from the negative set because very similar parts will end up in the negative set.



